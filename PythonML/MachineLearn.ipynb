{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning\n",
    "# Currently set for breast cancer data\n",
    "# make sure the data is classified numerically (e.g. 10 classifications = output column with 0,1,...,8,9)\n",
    "# Three algorithms shown here\n",
    "# 1. Linear Regression\n",
    "# 2. Logistic Regression\n",
    "# 3. Neural Network (can change number of hidden layers, nodes per hidden layer, etc)\n",
    "# 4. K-Means\n",
    "\n",
    "\n",
    "filename = r'C:\\Users\\Alan Horst\\MachineLearningNg\\PythonML\\data\\BreastCancerdata.csv'\n",
    "#filename = r'C:\\Users\\Alan Horst\\MachineLearningNg\\PythonML\\data\\ex1data2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "dataBC = pd.read_csv(filename)\n",
    "dataBC.head()\n",
    "\n",
    "Linear_Regression = 1\n",
    "Logistic_Regression = 1\n",
    "Neural_Network = 1\n",
    "K_Means = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class to Numeric Representation:\n",
      "{'M': [0], 'B': [1]}\n"
     ]
    }
   ],
   "source": [
    "#Choose Data to Input\n",
    "#DataInputx = ['area', 'occupants']\n",
    "DataInputx = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']\n",
    "dataBC_X = dataBC[DataInputx].copy()\n",
    "\n",
    "#Feature Scaling\n",
    "for i in range(0,len(DataInputx)):\n",
    "    mean = dataBC_X[DataInputx[i]].mean(axis=0)\n",
    "    std = dataBC_X[DataInputx[i]].std(axis=0)\n",
    "    def Norm(df):\n",
    "        return (df[DataInputx[i]]-mean)/std\n",
    "    dataBC_X[DataInputx[i]] = dataBC_X.apply(Norm, axis=1)\n",
    "dataBC_X['zeroeth'] = 1\n",
    "\n",
    "## Turns string classes into numerical ids ('classtonum' dictionary contains class to numeric translation)\n",
    "if Logistic_Regression == 1 or Neural_Network == 1 or kmeans == 1:\n",
    "    uniq = dataBC.diagnosis.unique()\n",
    "    classtonum = {k : [] for k in uniq}\n",
    "    for i in range(dataBC.diagnosis.nunique()):\n",
    "        classtonum[uniq[i]].append(i)\n",
    "    print('Class to Numeric Representation:')\n",
    "    print(classtonum)\n",
    "    dataBC.diagnosis = [int(classtonum[item][0]) for item in dataBC.diagnosis]\n",
    "\n",
    "dataBC_Y = dataBC[['diagnosis']].copy()\n",
    "Xtot = dataBC_X.values\n",
    "X = Xtot[0:400,:] #Learning Set\n",
    "ytot = dataBC_Y.values\n",
    "y = ytot[0:400,:]\n",
    "Xtest = Xtot[400:,:] #Test Set\n",
    "ytest = ytot[400:,:]\n",
    "n = len(X[0,:])\n",
    "m = len(y[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions reduced from 18 to 11\n",
      "['radius_mean', 'texture_mean', 'perimeter_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']\n"
     ]
    }
   ],
   "source": [
    "#Principal Component Algorithm (Reduces dimensions of data)\n",
    "def PCA(X):\n",
    "    sigma = 1/m * (X.conj().T @ X)\n",
    "    U,S,V =np.linalg.svd(sigma) #Least Squares\n",
    "    I = (abs(U[0]) >= 0.05)*1\n",
    "    Im = DataInputx\n",
    "    for j in range(len(I)):\n",
    "        if I[j] == 0:\n",
    "            Im[j] = 'Unimp'\n",
    "    Im = [item for item in Im if item != 'Unimp']\n",
    "    i = 1\n",
    "    while 1-sum(S[0:i])/sum(S) >= .01: #Continues until varance is small enough (1%) from true data\n",
    "        if 1-sum(S[:-1])/sum(S) >= .01:\n",
    "            print(\"No Reduction Found\")\n",
    "            Z = X\n",
    "            break\n",
    "        i+=1\n",
    "        if 1-sum(S[0:i])/sum(S) <= .01:\n",
    "            K = i\n",
    "            print(\"Dimensions reduced from %d to %d\" % (n,K))\n",
    "            #print(\"Unreduced features that contributed at least 20% to most important reduced feature...\")\n",
    "            print(Im)\n",
    "            Z = X @ U[:, 0:K] #New reduced dataset Z to replace X\n",
    "            Ztest = Xtest @ U[:, 0:K] #Apply same reduction to test set (does not perform the reduction using this data)\n",
    "            break\n",
    "    return Z,Ztest\n",
    "#Comment this out if you do not want to reduce \n",
    "X,Xtest = PCA(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F-Score\n",
    "# An f-score often gives a better idea of how accurate our prediction is, especially if there are few positive examples\n",
    "def fscore(pred,y):\n",
    "    diff = y - 2*pred\n",
    "    tp = sum(diff == -1) #true positives\n",
    "    fp = sum(diff == -2) #false positives\n",
    "    fn = sum(diff == 1)  # false negatives\n",
    "    prec = tp/(tp+fp)    # precision\n",
    "    rec = tp/(tp+fn)     # recall\n",
    "    Fscore = 2*prec*rec/(prec+rec) #fscore\n",
    "    return Fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X[0,:]) #Reinitialize n, m from reduced data\n",
    "m = len(y[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta Result:\n",
      "[[ 0.11328797]\n",
      " [-0.0347924 ]\n",
      " [-0.20043063]\n",
      " [ 0.52723893]\n",
      " [-0.06016648]\n",
      " [ 0.18725119]\n",
      " [ 0.04275894]\n",
      " [ 0.06386074]\n",
      " [ 0.05342147]\n",
      " [-0.10847571]\n",
      " [-0.07918015]]\n",
      "Algorithm Test Result:\n",
      "[0.00025187]\n",
      "Actual Result\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "if Linear_Regression == 1:\n",
    "    # Some gradient descent settings\n",
    "    num_iters = 1500\n",
    "    alpha = 0.01\n",
    "    theta = np.zeros((n,1))\n",
    "    J = 1/(2*m) * sum(np.square((X.dot(theta))-y)) #cost function\n",
    "        \n",
    "    J_history = []\n",
    "    iter = 0\n",
    "    while iter < num_iters:\n",
    "        temp = theta\n",
    "        theta = temp - (alpha/(m) * sum(((X.dot(theta)-y).T).dot(X))).reshape((-1, 1)) #find optimal theta (weight)\n",
    "        #np.append(J_history, computeCost(X, y, theta))\n",
    "        iter += 1\n",
    "    print('Theta Result:')\n",
    "    print(theta)\n",
    "    test = X[3]\n",
    "    print('Algorithm Test Result:')\n",
    "    print(np.dot(theta.T,test))\n",
    "    print('Actual Result')\n",
    "    print(y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test set 95.857988 % accurate.\n",
      "Fscore: 0.972332\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "regularization = 1 #To minimize effect of overfitting, set = 1\n",
    "if Logistic_Regression == 1:\n",
    "    mtest = len(ytest[:,0])\n",
    "    iterations = 1500;\n",
    "    alpha = 0.01;\n",
    "    theta = np.zeros(n)\n",
    "    initial_theta = np.zeros((n,1))\n",
    "    def sigmoid(x): #returns sigmoid, numbers >> 0 give 1, numbers << 0 give 0\n",
    "        return 1/ (1 + np.exp(-x))\n",
    "    def h(x, theta): #predicted output\n",
    "        return np.array(sigmoid(np.dot(x, theta)))\n",
    "    sigmoid = np.vectorize(sigmoid)\n",
    "    def costFunction(theta,x,y, lambda_):\n",
    "\n",
    "        p1 = np.dot(y.T, np.log(h(x, theta)).reshape(m,1))\n",
    "        p2 = np.dot((np.ones((m,1)) - y).T, np.log( 1 - h(x, theta)).reshape(m,1))\n",
    "        summ = (p1 + p2)\n",
    "        J = -summ[0]/m #cost function\n",
    "        if regularization == 1: #regularizes the cost, to minimize possibly uneeded/overfitting theta\n",
    "            reg_J = lambda_/(2*m)*sum(np.square(theta[1:]))\n",
    "            J[1:] = J[1:] + reg_J\n",
    "\n",
    "\n",
    "        return J\n",
    "    def gradientVect(theta, x, y, lambda_): #computes gradient for optimization\n",
    "        gradient = []\n",
    "\n",
    "        theta = theta.reshape(n,1)\n",
    "\n",
    "        beta = h(x, theta) - y #prediction vs actual\n",
    "        \n",
    "        grad = np.dot(X.T, beta) * 1./m\n",
    "\n",
    "        if regularization == 1: #regularization for grad\n",
    "            reg = theta[1:] * lambda_/m\n",
    "            grad[1:] = grad[1:] + reg\n",
    "\n",
    "        return grad.flatten()\n",
    "        \n",
    "    def optimizeTheta(x, y, nLabels, lambda_):\n",
    "\n",
    "        for i in np.arange(0, nLabels):\n",
    "            theta = np.zeros((n,1))\n",
    "            #scipy minimize takes the cost, gradient, theta and our data and finds an optimal minimum\n",
    "            res = minimize(costFunction, theta, args=(x, (y == i)*1, lambda_), method='BFGS',\n",
    "                       jac=gradientVect, options={'maxiter':300})\n",
    "        return res\n",
    "    res = optimizeTheta(X,y,1,0.5) #learning parameter lambda set to 0.5, but good to iterate and check many lambda vals\n",
    "    pred = np.around(sigmoid(Xtest @ (res.x)[:, None])) #rounds up or down from 0.5, depending on problem this may need changing\n",
    "    pred=1-pred\n",
    "    diff = pred - ytest\n",
    "    fin = 100*(1-np.count_nonzero(diff)/mtest)\n",
    "    fs = fscore(pred, ytest)\n",
    "    print(\"Predictions on test set %f %% accurate.\" %fin)\n",
    "    print(\"Fscore: %f\" %fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "regularization = 1 #To minimize effect of overfitting, set = 1\n",
    "if Neural_Network == 1:\n",
    "    iterations = 500\n",
    "    nodes = 10\n",
    "    alpha = 0.01\n",
    "    num_layers = 2 #This can be increased/decreased, keep complexity of data in mind, look out for overfitting/computation time\n",
    "    epsilon_init = 0.12\n",
    "    classes = len(np.unique(y))\n",
    "    Y = np.zeros((classes,m))\n",
    "    \n",
    "    for i in range(0,m-1): #turns our output into vectors for each example\n",
    "        ytemp = np.zeros((classes,1))\n",
    "        position = [y[i]]\n",
    "        ytemp[position] = 1\n",
    "        Y[:,i] = ytemp[:,0]\n",
    "        \n",
    "    #This randomizes the int weights, otherwise the neural network will have no basis to start (no diff between each node)\n",
    "    def randInitWeights(L_out,L_in):\n",
    "        return np.random.rand(L_out,L_in)*2*epsilon_init-epsilon_init\n",
    "    \n",
    "    theta = []\n",
    "    theta_init = []\n",
    "    for i in range(0,num_layers+1): #we have to initialize the layers to match our chosen structure and data\n",
    "        if i == 0:\n",
    "            theta = [np.zeros((nodes,n))] #matches the data features and the nodes of first layer\n",
    "            timatx = randInitWeights(nodes,n)\n",
    "            theta_init = [timatx]\n",
    "        elif i == num_layers:\n",
    "            tmatx = np.zeros((classes,nodes+1)) #matches the number of output classes and nodes from last layer\n",
    "            theta.append(tmatx)\n",
    "            timatx = randInitWeights(classes,nodes+1)\n",
    "            theta_init.append(timatx)\n",
    "        else:\n",
    "            tmatx = np.zeros((nodes,nodes+1)) #hidden layer interactions (notice added bias node)\n",
    "            theta.append(tmatx)\n",
    "            timatx = randInitWeights(nodes,nodes+1)\n",
    "            theta_init.append(timatx)\n",
    "\n",
    "            \n",
    "    def sigmoid(x): #sigmoid function again\n",
    "        return 1/ (1 + np.exp(-x))\n",
    "    sigmoid = np.vectorize(sigmoid)\n",
    "    \n",
    "    def sigmoid_grad(x): #derivative of sigmoid function for grad\n",
    "        return np.dot(sigmoid(x), (1-sigmoid(x)))\n",
    "    sigmoid_grad = np.vectorize(sigmoid_grad)\n",
    "    \n",
    "    def unroll(theta): #unrolls the weights into a long vector (not used here, but often needed for min function)\n",
    "        for n in range(0,num_layers+1):\n",
    "            if n == 0:\n",
    "                unrolled = (theta[n].flatten())\n",
    "            else:\n",
    "                unrolled = np.append(unrolled,theta[n].flatten())\n",
    "        return unrolled\n",
    "    \n",
    "    def fwd(theta,x,y,n,i): #forward propogation (makes a guess)\n",
    "        if n == 0:\n",
    "            return np.insert(sigmoid(theta[n]@x[i,:].conj().T),0,1)\n",
    "        elif n==num_layers:\n",
    "            return sigmoid(theta[n]@fwd(theta,x,y,n-1,i))\n",
    "        else:\n",
    "            return np.insert(sigmoid(theta[n]@fwd(theta,x,y,n-1,i)),0,1)\n",
    "        \n",
    "    def costFunction(theta,x,y,lambda_): #computes the cost using fwd prop\n",
    "        summ=0\n",
    "        for i in range(0,m):\n",
    "            p1 = np.dot(y[:,i], np.log(fwd(theta,x,y,num_layers,i)))\n",
    "            p2 = np.dot((1 - y[:,i]),np.log(1 - fwd(theta,x,y,num_layers,i)))\n",
    "            temp = (p1 + p2)\n",
    "            summ += temp\n",
    "        J = -summ/m\n",
    "        if regularization == 1: #applies regularization to reduce overfitting\n",
    "            sumreg = 0\n",
    "            for i in range(1,num_layers):\n",
    "                sumreg += sum(np.square(theta[i]))\n",
    "            reg_J = lambda_/(2*m)*(sumreg)\n",
    "            J = J + reg_J\n",
    "        return J\n",
    "    \n",
    "    def bck(theta,x,y,n,i): #back propogation (sends our errors back through the network)\n",
    "        if n == 0:\n",
    "            return (fwd(theta,x,y,num_layers,i) - y[:,i])\n",
    "        elif n == num_layers:\n",
    "            g_z = np.insert(sigmoid_grad(theta[0]@x[i,:]),0,1)\n",
    "            d = theta[(num_layers+1)-n].conj().T@bck(theta,x,y,n-1,i)*g_z\n",
    "            return d[1:]\n",
    "        else:\n",
    "            g_z = np.insert(sigmoid_grad(theta[(num_layers)-n]@fwd(theta,x,y,(num_layers-1)-n,i)),0,1)\n",
    "            d = theta[(num_layers+1)-n].conj().T@bck(theta,x,y,n-1,i)*g_z\n",
    "            return d[1:]    \n",
    "        \n",
    "    def gradientVect(theta, x, y, lambda_): #computes the gradient for optimization\n",
    "        gradient = []\n",
    "        theta_grad = []\n",
    "        reg_theta = []\n",
    "        for j in range(0,num_layers+1):\n",
    "            for i in range(0,m):        \n",
    "                    if j == 0:\n",
    "                        if i == 0:\n",
    "                            theta_grad = [np.outer(bck(theta,x,y,num_layers,i), x[i,:].conj().T)]\n",
    "                        else:\n",
    "                            theta_grad[0] += np.outer(bck(theta,x,y,num_layers,i), x[i,:].conj().T)\n",
    "                    else:\n",
    "                        if i == 0:\n",
    "                            theta_grad.append(np.outer(bck(theta,x,y,(num_layers)-(j),i), fwd(theta,x,y,j-1,i).conj().T))\n",
    "                        else:\n",
    "                            theta_grad[j] += np.outer(bck(theta,x,y,(num_layers)-(j),i), fwd(theta,x,y,j-1,i).conj().T)\n",
    "        for i in range(0,num_layers+1):\n",
    "            theta_grad[i] = 1/m*(theta_grad[i])\n",
    "            if regularization == 1:\n",
    "                if i == 0:\n",
    "                    reg_theta = [lambda_/m*(theta[i])]\n",
    "                    theta_grad[i][:,1:] += reg_theta[i][:,1:]\n",
    "                else:\n",
    "                    reg_theta.append(lambda_/m*(theta[i]))\n",
    "                    theta_grad[i][:,1:] += reg_theta[i][:,1:]\n",
    "        grad = theta_grad\n",
    "        return grad\n",
    "    \n",
    "    theta_grad = gradientVect(theta_init, X, Y, 0.9)\n",
    "    initial_nn_params = unroll(theta_init)\n",
    "\n",
    "    def optimizeTheta(x, y, theta, alpha, num_iters, lambda_): #basic gradient descent to find minima\n",
    "        J_history = []\n",
    "        for i in range(num_iters):\n",
    "            cost = costFunction(theta,x,y,lambda_)\n",
    "            theta_grad = gradientVect(theta, x, y, lambda_)\n",
    "            theta_fin = theta\n",
    "            J_history.append(cost)\n",
    "            for n in range(0,num_layers+1):\n",
    "                    theta_fin[n] = theta_fin[n] - (alpha*theta_grad[n])\n",
    "        return theta_fin , J_history\n",
    "                \n",
    "    nnTheta, nnJ_history = optimizeTheta(X,Y,theta_init,0.8,400,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Set Accuracy...\n",
      "97.63313609467455\n",
      "Fscore: 0.950000\n"
     ]
    }
   ],
   "source": [
    "if Neural_Network == 1:\n",
    "    z = len(ytest[:,0])\n",
    "    Ytest = np.zeros((classes,z))\n",
    "    \n",
    "    for i in range(0,z-1): #vectorizes ytest output\n",
    "        ytemp = np.zeros((classes,1))\n",
    "        position = [ytest[i]]\n",
    "        ytemp[position] = 1\n",
    "        Ytest[:,i] = ytemp[:,0]\n",
    "    \n",
    "    def predict(theta,X): #uses the test data to see how well our network did\n",
    "        h = []\n",
    "        X0 = np.ones((z,1))\n",
    "        for n in range(0,num_layers+1):\n",
    "            if n == num_layers:\n",
    "                h = sigmoid(h @ theta[n].conj().T)\n",
    "            elif n==0:\n",
    "                h = sigmoid(X @ theta[n].conj().T)\n",
    "                h = np.hstack((X0,h))\n",
    "            else:\n",
    "                h = sigmoid(h @ theta[n].conj().T)\n",
    "                h = np.hstack((X0,h))\n",
    "        return h\n",
    "    pred = predict(nnTheta,Xtest)\n",
    "    prediction = np.around(pred)[:,0]\n",
    "    yin = Ytest[0,:]\n",
    "    out = yin == prediction\n",
    "    fin = np.mean(1*out) * 100 #percentage of examples in dataset we predicted correctly.\n",
    "    fs = fscore(prediction, yin)\n",
    "    print('Test Data Set Accuracy...')\n",
    "    print(fin)\n",
    "    print(\"Fscore: %f\" %fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on test set 90.500000 % accurate.\n",
      "Fscore: 0.920502\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#K-means\n",
    "import matplotlib.pyplot as plt\n",
    "if K_Means == 1:\n",
    "    k=2\n",
    "    idx = np.zeros(m)\n",
    "    temp = np.zeros(k)\n",
    "    idex = np.random.randint(m, size=k)\n",
    "    ctds = X[idex,:]\n",
    "    def closestctd(X, ctds):\n",
    "        for i in range(0,m):\n",
    "            for j in range(0,k):\n",
    "                temp[j] = np.linalg.norm(X[i,:] - ctds[j,:])**2\n",
    "            I = list(temp).index(min(temp))\n",
    "            idx[i] = I\n",
    "        return idx\n",
    "    def compctds(X, idx, k):\n",
    "        for i in range(0,k):\n",
    "            ck = (idx == i).sum()\n",
    "            B = (idx == i).astype(int)\n",
    "            Xi = X * B[:, None]\n",
    "            ctds[i,:] = 1/ck * sum(Xi,1)\n",
    "        return ctds\n",
    "    def runKmeans(X, ctds, iter):\n",
    "        ctds = ctds\n",
    "        previous = ctds\n",
    "        for i in range(0,iter):\n",
    "            idx = closestctd(X, ctds)\n",
    "            plotprogkmeans(X, ctds, previous, idx, k, i)\n",
    "            ctds = compctds(X, idx, k)\n",
    "        return ctds,idx\n",
    "    def plotprogkmeans(X, ctds, previous, idx, K, i): #Markers show how centroids are updated over each iteration.\n",
    "        colore = {0.0 : 'red', 1.0 : 'lime'}\n",
    "        colors = np.vectorize(colore.__getitem__)(idx.T)\n",
    "        plt.scatter(X[:,0],X[:,1],color = colors, zorder=1)\n",
    "        plt.scatter(ctds[:,0], ctds[:,1], s=200, marker='+', linewidth=5, color='black', zorder=1+i)\n",
    "    ctds,idx = runKmeans(X, ctds, 40)\n",
    "    idx.shape = (m,1)\n",
    "    pred = y - idx\n",
    "    fin = 100*np.count_nonzero(pred)/m\n",
    "    idx = 1 - idx\n",
    "    fs = fscore(idx, y)\n",
    "    print(\"Predictions on test set %f %% accurate.\" %fin)\n",
    "    print(\"Fscore: %f\" %fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
